#!/usr/bin/ env python3

import requests
import re
from urllib.parse import urljoin
from bs4 import BeautifulSoup




class Scanner:
    def __init__(self,url,ignore_links):
        self.session=requests.Session()
        self.target_url=url
        self.target_links=[]
        self.links_to_ignore=ignore_links

    def extract_links(self,url):
        # use the session for login websites
        response = self.session.get(url)
        # find all links in href
        return re.findall(r'(?:href=")(.*?)"', str(response.content))

    def crawl(self,url=None):
        if url==None:
            url=self.target_url

        href_links = self.extract_links(url)

        for link in href_links:
            # for relative links
            link = urljoin(url, link)
            if "#" in link:
                link = link.split("#")[0]

            # some links can be not related to domain
            if self.target_url in link and link not in self.target_links and link not in self.links_to_ignore:
                self.target_links.append(link)
                print(link)
                # using the function recursively allows to map the entire website
                self.crawl(link)

    def extract_forms(self,url):
        response = self.session.get(url)
        # use BeautifulSoup to parse the html
        parsed_html = BeautifulSoup(response.content)
        return parsed_html.findAll("form")

    def submit_forms(self,form,value,url):
        # get the action the form is doing
        action = form.get("action")
        # create an url to post the fork
        post_url = urljoin(url, action)
        # get the method
        method = form.get("method")
        # get all the input fields
        inputs_list = form.findAll("input")

        # dictionary to post
        post_data = {}

        for input in inputs_list:
            # get the name of the input
            input_name = input.get("name")
            # get the type of the input
            input_type = input.get("type")
            input_value = input.get("value")

            if input_type == "text":
                input_value = value
            # set the value of the name in dict
            post_data[input_name] = input_value
            if method=="post":
                # send the form with the created url and created dict as data
                return self.session.post(post_url, data=post_data)
            # if method ==get
            return self.session.get(post_url, params=post_data )

    def run_scanner(self):

        for link in self.target_links:
            forms=self.extract_forms(link)
            # iterate over forms list
            for form in forms:
                print("[+] Testing form in :"+link)
                is_vulnerable_to_xss=self.test_xss_in_form(form,link)
                if is_vulnerable_to_xss:
                    print("\n\n[+] Discovered XSS in "+link + "form: " )
                    print(form)


            # if = then the link sends data
            if "=" in link:
                print("[+] Testing: "+link)
                is_vulnerable_to_xss=self.text_xss_in_link(link)
                if is_vulnerable_to_xss:
                    print("[+] Discovered XSS in "+link)

    def test_xss_in_form(self, form, url):
        # js to inject
        xss_test_script='<sCript>alert("test") </scriPt>'
        response=self.submit_forms(form,xss_test_script,url)
        # if the injected js is in the response the site is vuln
        if xss_test_script in response.content:
            return True

    def text_xss_in_link(self,url):
        # js to inject
        xss_test_script = '<sCript>alert("test") </scriPt>'

        # create a new url with the injected js
        url=url.replace("=","="+xss_test_script)
        response=self.session.get(url)
        if xss_test_script in response.content:
            return True